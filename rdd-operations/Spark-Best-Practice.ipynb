{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark best practices\n",
    "source : https://robertovitillo.com/2015/06/30/spark-best-practices/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rdd = sc.textFile('derby.log') \\\n",
    "        .flatMap(lambda line: line.split()) \\\n",
    "        .map(lambda word: (word, 1)) \\\n",
    "        .reduceByKey(lambda x,y: x+y, 3) \\\n",
    "        .collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark UI\n",
    "\n",
    "http://localhost:4040\n",
    "## Use the right level of parallelism\n",
    "In general, 2-3 task per CPU core in your cluster are recommended.\n",
    "## Reduce working set size\n",
    "be careful with operations resulting too large working set for the following tasks.  \n",
    "Spark's shuffle operations(**sortByKey**, **groupByKey**, **reduceByKey**, **join**, etc) build a hash table within each task to perform the grouping, which can often be large.\n",
    "## Avoid groupByKey for associative operations\n",
    "both **reduceByKey** and **groupByKey** can be used for the same purposes but **reduceByKey** works much better on a large dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aviod reduceByKey when the input and output value types are different"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rdd = sc.textFile('derby.log') \\\n",
    "        .flatMap(lambda line: line.lower().split()) \\\n",
    "        .filter(lambda x: len(x) > 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'a', {u'8', u'p'}),\n",
       " (u'c', {u'l', u'o', u's'}),\n",
       " (u'i', {u'n'}),\n",
       " (u'm', {u'a'}),\n",
       " (u'-', {u'-'}),\n",
       " (u'3', {u'0'}),\n",
       " (u'o', {u'n', u'r', u's'}),\n",
       " (u'1', {u'0'}),\n",
       " (u's', {u'o', u't'}),\n",
       " (u'u', {u's'}),\n",
       " (u'/', {u'u'}),\n",
       " (u'w', {u'i'}),\n",
       " (u'b', {u'o'}),\n",
       " (u'd', {u'a', u'e', u'i'}),\n",
       " (u'f', {u'i', u'o', u'r'}),\n",
       " (u'(', {u'1'}),\n",
       " (u'j', {u'a'}),\n",
       " (u'l', {u'o'}),\n",
       " (u'0', {u'2'}),\n",
       " (u'2', {u'0'}),\n",
       " (u't', {u'h'}),\n",
       " (u'v', {u'e'})]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# not good\n",
    "rdd.map(lambda p: (p[0], {p[1]})) \\\n",
    "   .reduceByKey(lambda x,y: x|y) \\\n",
    "   .collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'a', {u'8', u'p'}),\n",
       " (u'c', {u'l', u'o', u's'}),\n",
       " (u'i', {u'n'}),\n",
       " (u'm', {u'a'}),\n",
       " (u'-', {u'-'}),\n",
       " (u'3', {u'0'}),\n",
       " (u'o', {u'n', u'r', u's'}),\n",
       " (u'1', {u'0'}),\n",
       " (u's', {u'o', u't'}),\n",
       " (u'u', {u's'}),\n",
       " (u'/', {u'u'}),\n",
       " (u'w', {u'i'}),\n",
       " (u'b', {u'o'}),\n",
       " (u'd', {u'a', u'e', u'i'}),\n",
       " (u'f', {u'i', u'o', u'r'}),\n",
       " (u'(', {u'1'}),\n",
       " (u'j', {u'a'}),\n",
       " (u'l', {u'o'}),\n",
       " (u'0', {u'2'}),\n",
       " (u'2', {u'0'}),\n",
       " (u't', {u'h'}),\n",
       " (u'v', {u'e'})]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# better way\n",
    "def seq_op(xs, x):\n",
    "    xs.add(x)\n",
    "    return xs\n",
    "\n",
    "def comb_op(xs, ys):\n",
    "    return xs | ys\n",
    "\n",
    "rdd.map(lambda p: (p[0], p[1])).aggregateByKey(set(), seq_op, comb_op).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Avoid the flatMap-join-groupBy pattern\n",
    "use **cogroup** instead\n",
    "## Python memory overhead\n",
    "set **spark.executor.memory** option\n",
    "## Use broadcast variables\n",
    "## Cache judiciously\n",
    "## Don't collect large RDDs\n",
    "use **take** or **takeSample**\n",
    "## Minimize amount of data shuffled\n",
    "it involves disk I/O, data serialization, network I/O and temporary storage.\n",
    "## Know the standard library\n",
    "## Use dataframes"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
